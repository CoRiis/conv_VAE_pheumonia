{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "dd92a1ecff915e0aa69a9a2613ef672228917283"
   },
   "outputs": [],
   "source": [
    "#### Defining loss functions ####\n",
    "from torch.nn.functional import binary_cross_entropy\n",
    "from torch import optim\n",
    "import math\n",
    "\n",
    "def Gaussian_density(sample_img,mu_img,log_var_img):\n",
    "    c = - 0.5 * math.log(2 * math.pi)\n",
    "    density = c - log_var_img/2 - (sample_img - mu_img)**2/(2 * torch.exp(log_var_img))\n",
    "    #density = c -  (sample_img - mu_img)**2/(2 * torch.exp(log_var_img))\n",
    "    #print(\"Density:\",density)\n",
    "    #print(\"Density.shape:\", density.shape)\n",
    "    return torch.sum(density,dim = 1) # Sum over channels\n",
    "\n",
    "def kl_a_calc(q_a,q_mu, q_log_var,p_mu, p_log_var):\n",
    "    # The function assumes: \n",
    "        # q_a has dimension: [batch_size,No_samples,latent_features]\n",
    "        # q_mu/log_var has dimension: [batch_size,latent_features]\n",
    "        # p_mu/log_var has dimension: [batch_size,No_samples,latent_features]\n",
    "        \n",
    "    p_mu      = p_mu.view(batch_size,num_samples,-1)\n",
    "    p_mu      = torch.mean(p_mu, dim = 1)\n",
    "    p_log_var = p_log_var.view(batch_size,num_samples,-1)\n",
    "    p_log_var = torch.mean(p_log_var, dim = 1)\n",
    "    \n",
    "    def log_gaussian(x, mu, log_var):\n",
    "        log_pdf = - 0.5 * math.log(2 * math.pi) - log_var / 2 - (x - mu)**2 / (2 * torch.exp(log_var))\n",
    "        log_pdf = torch.sum(log_pdf, dim=1) # sum over each over the observations (mu + log_var*epsilon)\n",
    "        log_pdf = torch.sum(log_pdf,dim=1) # sum over q_a, i.e. latent features\n",
    "        return log_pdf\n",
    "\n",
    "    # put in middle dimension\n",
    "    q_mu      = q_mu.unsqueeze(1)\n",
    "    q_log_var = q_log_var.unsqueeze(1)\n",
    "    p_mu      = p_mu.unsqueeze_(1)\n",
    "    p_log_var = p_log_var.unsqueeze_(1)\n",
    "    # densities of each disitribution \n",
    "    qz = log_gaussian(q_a,q_mu,q_log_var)\n",
    "    pz = log_gaussian(q_a,p_mu,p_log_var)\n",
    "    # kl divergence\n",
    "    kl = qz - pz\n",
    "    \n",
    "    return kl\n",
    "\n",
    "def ELBO_loss(sample_img, outputs, kl_warmup=None):\n",
    "    # Parameter in deterministic warmup for KL divergence\n",
    "    beta = 1 if kl_warmup is None else kl_warmup\n",
    "    \n",
    "    # Weighting kl's and likelihood\n",
    "    w1 = 0.5\n",
    "    w2 = 0.5\n",
    "    \n",
    "    if type(outputs['x_mean']) == list:\n",
    "        ELBO = []\n",
    "        kl_x = -0.5 * torch.sum(1 + outputs['log_var'] - outputs['mu']**2 - torch.exp(outputs['log_var']), dim=1)\n",
    "        kl_x = torch.sum(kl_x,dim=1) # sum over the features\n",
    "        for j in range(No_classes):\n",
    "            likelihood = Gaussian_density(sample_img, outputs['x_mean'][j], outputs['x_log_var'][j])\n",
    "            if aux_variables > 0:\n",
    "                kl_a = kl_a_calc(outputs[\"q_a\"],outputs[\"q_a_mu\"],outputs[\"q_a_log_var\"],outputs[\"p_a_mu\"][j],outputs[\"p_a_log_var\"][j])\n",
    "                kl = w1 * kl_x + (1 - w1) * kl_a\n",
    "            else:\n",
    "                kl_a = torch.Tensor([0])\n",
    "                kl = kl_x \n",
    "            likelihood = likelihood.view(batch_size, -1)\n",
    "            likelihood = torch.sum(likelihood, dim=1) # Sum over features (224x224 = 50,176)\n",
    "            ELBO.append(w2 * likelihood - (1 - w2) * beta * kl)\n",
    "        \n",
    "        L = torch.cat( (torch.unsqueeze(ELBO[0],1),torch.unsqueeze(ELBO[1],1)),dim =1 )\n",
    "        # Calculate entropy H(q(y|x)) and sum over all labels\n",
    "        logits = torch.mean(outputs['y_hat'],dim = 1)\n",
    "        \n",
    "        H = -torch.sum(torch.mul(logits, torch.log(logits + 1e-8)), dim=-1) \n",
    "        L = torch.sum(torch.mul(logits, L), dim=-1)\n",
    "        \n",
    "        # Equivalent to -U(x)\n",
    "        U = L - H\n",
    "        \n",
    "        #RMS_1 = torch.sqrt(torch.mean((sample_img - outputs['x_mean'][0])**2))\n",
    "        #RMS_2 = torch.sqrt(torch.mean((sample_img - outputs['x_mean'][1])**2))\n",
    "        #U = - torch.sum(RMS_1 + RMS_2)\n",
    "        #U = - ( torch.abs(sample_img - outputs['x_mean'][0])**2 + torch.abs(sample_img - outputs['x_mean'][0])**2)\n",
    "        \n",
    "        return -torch.mean(U), -torch.mean(H), -torch.mean(L),  (1 - w2) * beta * torch.mean(kl), -w2 * torch.mean(likelihood), w1*torch.mean(kl_x), (1-w1)*torch.mean(kl_a)\n",
    "    else:\n",
    "        likelihood = Gaussian_density(sample_img, outputs['x_mean'], outputs['x_log_var'])\n",
    "        kl_x = -0.5 * torch.sum(1 + outputs['log_var'] - outputs['mu']**2 - torch.exp(outputs['log_var']), dim=1)\n",
    "        if aux_variables > 0:\n",
    "            kl_a = kl_a_calc(outputs[\"q_a\"],outputs[\"q_a_mu\"],outputs[\"q_a_log_var\"],outputs[\"p_a_mu\"],outputs[\"p_a_log_var\"])\n",
    "            kl = w1 * torch.mean(kl_x) + (1 - w1) * torch.mean(kl_a)\n",
    "        else:\n",
    "            kl_a = torch.Tensor([0])\n",
    "            kl = torch.mean(kl_x)\n",
    "        likelihood = likelihood.view(batch_size, -1)\n",
    "        likelihood = torch.sum(likelihood, dim=1) # Sum over features (224x224 = 50,176)\n",
    "        ELBO = w2 * torch.mean(likelihood) - (1 - w2) * beta * kl    \n",
    "        # Notice minus sign as we want to maximise ELBO\n",
    "        return -ELBO, (1 - w2) * beta * kl, -w2 * torch.mean(likelihood), w1*torch.mean(kl_x), (1-w1)*torch.mean(kl_a)\n",
    "    \n",
    "    # Regularization error: \n",
    "    # Kulback-Leibler divergence between approximate posterior, q(z|x)\n",
    "    # and prior p(z) = N(z | mu, sigma*I).\n",
    "    \n",
    "    # In the case of the KL-divergence between diagonal covariance Gaussian and \n",
    "    # a standard Gaussian, an analytic solution exists. Using this excerts a lower\n",
    "    # variance estimator of KL(q||p)\n",
    "    # Combining the two terms in the evidence lower bound objective (ELBO) \n",
    "    # mean over batch \n",
    "\n",
    "# Define optimizer: The Adam optimizer works really well with VAEs.\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.001)\n",
    "loss_function = ELBO_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "7e691c43f146ec4fb7e8a5157b35d650ecb8fbef",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#### Test loss functions ####\n",
    "\n",
    "from torch.autograd import Variable\n",
    "import gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache\n",
    "\n",
    "x, y = next(iter(train_loader_labelled))\n",
    "u, _ = next(iter(train_loader))\n",
    "\n",
    "y_hot =  torch.zeros([batch_size,2], requires_grad=True)\n",
    "for i in range(len(y)):\n",
    "    y_hot[i] = torch.tensor([0, 1]) if y[i]==1 else torch.tensor([1, 0])\n",
    "\n",
    "x, y, y_hot = Variable(x), Variable(y), Variable(y_hot)\n",
    "if cuda:\n",
    "    # They need to be on the same device and be synchronized.\n",
    "    x, y, y_hot = x.cuda(device=0), y.cuda(device=0), y_hot.cuda(device=0)\n",
    "    u = u.cuda(device=0)\n",
    "\n",
    "outputs = net(u)    \n",
    "#loss, kl, likelihood = loss_function(u,outputs)\n",
    "elbo_u, elbo_H, elbo_L, kl_u, likelihood_u, kl_u_x, kl_u_a= loss_function(u,outputs)\n",
    "outputs = net(x,y_hot)\n",
    "\n",
    "x_hat = outputs[\"x_hat\"]\n",
    "mu, log_var = outputs[\"mu\"], outputs[\"log_var\"]\n",
    "mu_img, log_var_img = outputs[\"x_mean\"], outputs[\"x_log_var\"]\n",
    "z = outputs[\"z\"]\n",
    "logits = outputs[\"y_hat\"]\n",
    "y_hot = y_hot.unsqueeze(dim = 1).repeat(1,logits.shape[1],1)\n",
    "classification_loss = torch.sum(torch.abs(y_hot - logits))\n",
    "\n",
    "#loss, kl = loss_function(x, mu_img, log_var_img, torch.sum(mu,dim = 1), torch.sum(log_var,dim = 1))\n",
    "if 1 == 0:\n",
    "    loss, kl, likelihood, kl_l_x, kl_l_a = loss_function(u,outputs)\n",
    "    print('mu:          ',mu.shape,torch.sum(torch.isnan(mu)))\n",
    "    print('log_var:     ',log_var.shape,torch.sum(torch.isnan(log_var)))\n",
    "    print('mu_img:      ',mu_img.shape,torch.sum(torch.isnan(mu_img)))\n",
    "    print('log_var_img: ',log_var_img.shape,torch.sum(torch.isnan(log_var_img)))\n",
    "    print('x:           ',x.shape,torch.sum(torch.isnan(x)))\n",
    "    print('x_hat:       ',x_hat.shape,torch.sum(torch.isnan(x_hat)))\n",
    "    print('z:           ',z.shape,torch.sum(torch.isnan(z)))\n",
    "    print('Total loss:  ',loss)\n",
    "    print('kl:          ',kl)\n",
    "    print('Class. loss: ',classification_loss)\n",
    "    print('Likelihood:  ',likelihood)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
